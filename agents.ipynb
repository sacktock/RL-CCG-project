{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "agents.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLzqQdE7IKK4"
      },
      "source": [
        "# **Reinforcement Learning in Collectible Card Games**\n",
        "Below is the code for some of the AI agents described in the project paper. You will be able to find the code for all the agents however some of the previous iterations of these agents are omitted. You can play around yourself with the hyper-parameters and weights but note that the values given in this code were the final ones used in the paper.\n",
        "\n",
        "**Personal note:** massive shoutout to Vieira, Ronaldo and Chaimowicz, Luiz and Tavares, Anderson Rocha - the author's of gym-locm \"OpenAI Gym Environments for Legends of Code and Magic\". Both their code and reimplementation of the LOCM enviornment was vital for the success of this project. You can find it here https://github.com/ronaldosvieira/gym-locm ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVjt3PaTJZEr"
      },
      "source": [
        "*It is important to open drive and install gym-locm so that the AI can play games*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKbGHBGVr02T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80013818-5363-4670-d4bd-cf2fdc656949"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfeJ1dKQsBZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088e5f6b-8149-4a9e-f469-7f5756b43040"
      },
      "source": [
        "%cd drive/My Drive/locm_project\n",
        "! git clone https://github.com/ronaldosvieira/gym-locm.git\n",
        "%cd gym-locm\n",
        "! pip install -e ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/locm_project\n",
            "fatal: destination path 'gym-locm' already exists and is not an empty directory.\n",
            "/content/drive/My Drive/locm_project/gym-locm\n",
            "Obtaining file:///content/drive/My%20Drive/locm_project/gym-locm\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from gym-locm==1.0.0) (0.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gym-locm==1.0.0) (1.19.5)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from gym-locm==1.0.0) (2.1.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from gym-locm==1.0.0) (4.8.0)\n",
            "Collecting sty\n",
            "  Downloading https://files.pythonhosted.org/packages/57/20/8b633cdaa481bfeb5fd99eb3abbba151fe5d36ef64b2fb75868991f7e226/sty-1.0.0rc1-py3-none-any.whl\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-locm==1.0.0) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->gym-locm==1.0.0) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-locm==1.0.0) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->gym-locm==1.0.0) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from prettytable->gym-locm==1.0.0) (3.7.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->gym-locm==1.0.0) (0.7.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-locm==1.0.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->prettytable->gym-locm==1.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->prettytable->gym-locm==1.0.0) (3.4.1)\n",
            "Installing collected packages: sty, gym-locm\n",
            "  Running setup.py develop for gym-locm\n",
            "Successfully installed gym-locm sty-1.0.0rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GXZG76LJot_"
      },
      "source": [
        "*Imports*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT7kIJeYsFaR"
      },
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from operator import attrgetter\n",
        "from typing import Type\n",
        "\n",
        "import pexpect\n",
        "import sys\n",
        "\n",
        "from gym_locm.engine import *\n",
        "\n",
        "import gym\n",
        "import gym.spaces\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSgbKROHJsi4"
      },
      "source": [
        "**Random Battle Agent** - plays cards and picks actions randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ftAeRriJ0r0"
      },
      "source": [
        "class RandomBattleAgent():\n",
        "    def __init__(self, seed=None):\n",
        "        self.random = random.Random(seed)\n",
        "\n",
        "    def seed(self, seed):\n",
        "        self.random.seed(seed)\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        index = int(len(state.available_actions) * random.random())\n",
        "\n",
        "        return state.available_actions[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl9S6YfVJ3XP"
      },
      "source": [
        "**Greedy Battle Agent** - plays cards that maximize the value of a handcrafted heuristic (see paper for more details)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC8a7dRjKErH"
      },
      "source": [
        "class GreedyBattleAgent():\n",
        "    def seed(self, seed):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def eval_state(state):\n",
        "        \n",
        "        def eval_creature(creature):\n",
        "            score = 0\n",
        "\n",
        "            if creature.attack > 0:\n",
        "                score += 28.6875\n",
        "                score += creature.attack * 23.375\n",
        "                score += creature.defense * 29.625\n",
        "\n",
        "                if creature.has_ability('W'):\n",
        "                    score += creature.attack * 26\n",
        "\n",
        "                if creature.has_ability('L'):\n",
        "                    score += 29.0625\n",
        "\n",
        "            if creature.has_ability('G'):\n",
        "                score += 25.125\n",
        "\n",
        "            return score\n",
        "        \n",
        "        score = 0\n",
        "\n",
        "        pl = state.current_player\n",
        "        op = state.opposing_player\n",
        "\n",
        "        if pl.health < 5:\n",
        "            score -= 100\n",
        "\n",
        "        # check opponent's death\n",
        "        if op.health <= 0:\n",
        "            score += 100000\n",
        "\n",
        "        # check own death\n",
        "        elif pl.health <= 0:\n",
        "            score -= 100000\n",
        "\n",
        "        # health difference\n",
        "        score += (pl.health - op.health) * 2\n",
        "        \n",
        "        for c in pl.hand:\n",
        "            if not isinstance(c, Creature):\n",
        "                score += 14.6875\n",
        "\n",
        "        if len(pl.hand) + pl.bonus_draw + 1 <= 8:\n",
        "            score += (pl.bonus_draw + 1) * 20.625\n",
        "                \n",
        "        for pl_lane, op_lane in zip(pl.lanes, op.lanes):\n",
        "            # creature strength\n",
        "            score += sum(eval_creature(c) for c in pl_lane)\n",
        "            score -= sum(eval_creature(c) for c in op_lane)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def act(self, state):\n",
        "        best_action, best_score = None, float(\"-inf\")\n",
        "\n",
        "        if len(state.available_actions) == 1:\n",
        "            return state.available_actions[0]\n",
        "        \n",
        "        for action in state.available_actions:\n",
        "            state_copy = state.clone()\n",
        "            state_copy.act(action)\n",
        "\n",
        "            score = self.eval_state(state_copy)\n",
        "            if score > best_score:\n",
        "                best_action, best_score = action, score\n",
        "        return best_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0eY1UBJKNlE"
      },
      "source": [
        "*Datastructure required for hashing of game states*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNxF-3jQKXYw"
      },
      "source": [
        "class Node:\n",
        "    def __init__(self, state, actions, parent):\n",
        "        self.state = state\n",
        "        self.actions = actions\n",
        "        self.parent = parent\n",
        "        self.player_id = state.opposing_player.id if actions[:-1].count(Action(ActionType.PASS)) % 2 else state.current_player.id \n",
        "\n",
        "        self._hash = None\n",
        "        \n",
        "    def __hash__(self):\n",
        "        \"\"\"Nodes must be hashable\"\"\"\n",
        "        if self._hash is not None:\n",
        "            return self._hash\n",
        "\n",
        "        s = self.state\n",
        "        p0, p1 = self.state.players\n",
        "        cp = self.state.players[self.player_id]\n",
        "\n",
        "        attributes = [\n",
        "            s.phase, s.turn, s.current_player.id,\n",
        "            p0.health, p0.base_mana + p0.bonus_mana, p0.bonus_draw,\n",
        "            p1.health, p1.base_mana + p1.bonus_mana, p1.bonus_draw\n",
        "        ]\n",
        "\n",
        "        attributes.extend(c.instance_id\n",
        "                          for c in sorted(cp.hand, key=attrgetter('id')))\n",
        "\n",
        "        for p in (p0, p1):\n",
        "            for j in range(2):\n",
        "                for i in range(3):\n",
        "                    if len(p.lanes[j]) > i:\n",
        "                        c = sorted(p.lanes[j], key=attrgetter('id'))[i]\n",
        "\n",
        "                        stats = [c.instance_id, c.attack, c.defense] + \\\n",
        "                            list(map(int, map(c.keywords.__contains__, 'BCDGLW'))) + \\\n",
        "                            [int(p.id), j, c.able_to_attack()]\n",
        "                    else:\n",
        "                        stats = [-1] * 12\n",
        "\n",
        "                    attributes.extend(stats)\n",
        "\n",
        "        for action in self.actions:\n",
        "            attributes.extend((action.type, action.origin, action.target))\n",
        "\n",
        "        self._hash = hash(tuple(attributes))\n",
        "\n",
        "        return self._hash\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        \"\"\"Nodes must be comparable\"\"\"\n",
        "        return hash(self) == hash(other)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JigsaOq1KkJX"
      },
      "source": [
        "*Some additional helper functions*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jrc7lRFKjOe"
      },
      "source": [
        "def eval_state(state, pl, op):\n",
        "    def eval_creature(creature):\n",
        "        score = 0\n",
        "\n",
        "        if creature.attack > 0:\n",
        "            score += 20\n",
        "            score += creature.attack * 10\n",
        "            score += creature.defense * 5\n",
        "\n",
        "            if creature.has_ability('W'):\n",
        "                score += creature.attack * 5\n",
        "\n",
        "            if creature.has_ability('L'):\n",
        "                score += 20\n",
        "\n",
        "        if creature.has_ability('G'):\n",
        "            score += 9\n",
        "\n",
        "        return score\n",
        "                \n",
        "    score = 0\n",
        "\n",
        "    if pl.health < 5:\n",
        "        score -= 100\n",
        "\n",
        "        # check opponent's death\n",
        "    if op.health <= 0:\n",
        "        score += 100000\n",
        "\n",
        "        # check own death\n",
        "    elif pl.health <= 0:\n",
        "        score -= 100000\n",
        "\n",
        "        # health difference\n",
        "    score += (pl.health - op.health) * 2\n",
        "                \n",
        "    for c in pl.hand:\n",
        "        if not isinstance(c, Creature):\n",
        "            score += 21\n",
        "\n",
        "    if len(pl.hand) + pl.bonus_draw + 1 <= 8:\n",
        "        score += (pl.bonus_draw + 1) * 5\n",
        "                        \n",
        "    for pl_lane, op_lane in zip(pl.lanes, op.lanes):\n",
        "        # creature strength\n",
        "        score += sum(eval_creature(c) for c in pl_lane)\n",
        "        score -= sum(eval_creature(c) for c in op_lane)\n",
        "\n",
        "    return score\n",
        "\n",
        "def eval_node(node):\n",
        "    state_copy = node.state.clone()\n",
        "\n",
        "    actions = node.actions[:-1] if node.actions[-1] == Action(ActionType.PASS) else node.actions\n",
        "\n",
        "    for action in actions:\n",
        "        state_copy.act(action)\n",
        "\n",
        "    return eval_state(state_copy, state_copy.current_player, state_copy.opposing_player)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-b0ow60Ko_l"
      },
      "source": [
        "**Minimax tree search agent** - details of the play policy can be found in the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLDYk0tKKyo-"
      },
      "source": [
        "class TreeSearch:\n",
        "\n",
        "    def __init__(self, player_id, simulations=10):\n",
        "        self.agent = GreedyBattleAgent()\n",
        "        self.Q = defaultdict(float)  # total reward of each node\n",
        "        self.children = defaultdict(list)\n",
        "        self.player_id = player_id\n",
        "        self.simulations = simulations\n",
        "\n",
        "    def choose(self, state):\n",
        "        # choose an action in the real game\n",
        "        node = Node(state, [], None)\n",
        "\n",
        "        def score(n):\n",
        "            return self.Q[n]\n",
        "\n",
        "        return max(self.children[node], key=score).actions[-1]\n",
        "            \n",
        "    def search(self, state):\n",
        "        node = Node(state, [], None)\n",
        "\n",
        "        self._expand(node)\n",
        "        \n",
        "        for child in self.children[node]:\n",
        "            total_reward = 0\n",
        "            for _ in range(self.simulations):\n",
        "                new_state = child.state.clone()\n",
        "                total_reward += self._simulate(new_state)\n",
        "            self.Q[child] = total_reward / self.simulations\n",
        "        \n",
        "    def _expand(self, node):\n",
        "        # expand a node\n",
        "        if node in self.children:\n",
        "            return  # already expanded\n",
        "        \n",
        "        children = []\n",
        "\n",
        "        for action in node.state.available_actions:\n",
        "            new_state = node.state.clone()\n",
        "            new_state.act(action)\n",
        "            children.append(Node(new_state, node.actions + [action], node))\n",
        "\n",
        "        self.children[node] = children\n",
        "\n",
        "    def _simulate(self, new_state):\n",
        "        # simulate opponent move\n",
        "\n",
        "        amount_deck = len(new_state.opposing_player.deck)\n",
        "        amount_hand = len(new_state.opposing_player.hand)\n",
        "\n",
        "        ids = map(attrgetter('instance_id'),\n",
        "                  new_state.opposing_player.hand +\n",
        "                  new_state.opposing_player.deck)\n",
        "\n",
        "        new_deck = []\n",
        "\n",
        "        for i, id in zip(range(amount_deck + amount_hand), ids):\n",
        "            random_index = int(3 * random.random())\n",
        "\n",
        "            card = new_state._draft_cards[i][random_index].make_copy(id)\n",
        "\n",
        "            new_deck.append(card)\n",
        "\n",
        "        random.shuffle(new_deck)\n",
        "\n",
        "        new_state.opposing_player.deck = new_deck\n",
        "        new_state.opposing_player.hand = []\n",
        "\n",
        "        new_state.opposing_player.draw(amount=amount_hand)\n",
        "\n",
        "        for player in new_state.players:\n",
        "            random.shuffle(player.deck)\n",
        "\n",
        "        def eval_state(state, pl, op):\n",
        "            def eval_creature(creature):\n",
        "                score = 0\n",
        "\n",
        "                if creature.attack > 0:\n",
        "                    score += 20\n",
        "                    score += creature.attack * 10\n",
        "                    score += creature.defense * 5\n",
        "\n",
        "                    if creature.has_ability('W'):\n",
        "                        score += creature.attack * 5\n",
        "\n",
        "                    if creature.has_ability('L'):\n",
        "                        score += 20\n",
        "\n",
        "                if creature.has_ability('G'):\n",
        "                    score += 9\n",
        "\n",
        "                return score\n",
        "                \n",
        "            score = 0\n",
        "\n",
        "            if pl.health < 5:\n",
        "                score -= 100\n",
        "\n",
        "            # check opponent's death\n",
        "            if op.health <= 0:\n",
        "                score += 100000\n",
        "\n",
        "            # check own death\n",
        "            elif pl.health <= 0:\n",
        "                score -= 100000\n",
        "\n",
        "            # health difference\n",
        "            score += (pl.health - op.health) * 2\n",
        "                \n",
        "            for c in pl.hand:\n",
        "                if not isinstance(c, Creature):\n",
        "                    score += 21\n",
        "\n",
        "            if len(pl.hand) + pl.bonus_draw + 1 <= 8:\n",
        "                score += (pl.bonus_draw + 1) * 5\n",
        "                        \n",
        "            for pl_lane, op_lane in zip(pl.lanes, op.lanes):\n",
        "                # creature strength\n",
        "                score += sum(eval_creature(c) for c in pl_lane)\n",
        "                score -= sum(eval_creature(c) for c in op_lane)\n",
        "\n",
        "            return score\n",
        "        \n",
        "        while new_state.current_player.id == self.player_id:\n",
        "            action = self.agent.act(new_state)\n",
        "            new_state.act(action)\n",
        "            if new_state.winner is not None:\n",
        "                return eval_state(new_state, new_state.current_player, new_state.opposing_player)\n",
        "\n",
        "        while new_state.current_player.id != self.player_id:\n",
        "            # opponent greedily makes action\n",
        "            action = self.agent.act(new_state)\n",
        "            new_state.act(action)\n",
        "            if new_state.winner is not None:\n",
        "                return eval_state(new_state, new_state.opposing_player, new_state.current_player)\n",
        "\n",
        "        while new_state.current_player.id == self.player_id:\n",
        "            # tree search greedily makes action\n",
        "            action = self.agent.act(new_state)\n",
        "            new_state.act(action)\n",
        "            if new_state.winner is not None:\n",
        "                return eval_state(new_state, new_state.current_player, new_state.opposing_player)\n",
        "\n",
        "        while new_state.current_player.id != self.player_id:\n",
        "            # opponent greedily makes action\n",
        "            action = self.agent.act(new_state)\n",
        "            new_state.act(action)\n",
        "            if new_state.winner is not None:\n",
        "                return eval_state(new_state, new_state.opposing_player, new_state.current_player)\n",
        "\n",
        "        while new_state.current_player.id == self.player_id:\n",
        "            # tree search greedily makes action\n",
        "            action = self.agent.act(new_state)\n",
        "            new_state.act(action)\n",
        "            if new_state.winner is not None:\n",
        "                return eval_state(new_state, new_state.current_player, new_state.opposing_player)\n",
        "    \n",
        "        return eval_state(new_state, new_state.opposing_player, new_state.current_player)\n",
        "\n",
        "class TreeSearchBattleAgent():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def seed(self, seed):\n",
        "        pass\n",
        "    \n",
        "    def reset(self):\n",
        "        pass\n",
        "    \n",
        "    def act(self, state):\n",
        "        searcher = TreeSearch(state.current_player.id)\n",
        "\n",
        "        if len(state.available_actions) == 1:\n",
        "            return state.available_actions[0]\n",
        "\n",
        "        searcher.search(state)\n",
        "        return searcher.choose(state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsjlnazyK6C1"
      },
      "source": [
        "**(best) Monte-Carlo Tree-Search agent** - standard MCTS algorithm that uses the following modifications (details of which can be found in the paper): move ordering, probabalistic early cutoff simulation, progressive UCT.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZZhAgT3sLfO"
      },
      "source": [
        "class BestMCTS:\n",
        "\n",
        "    def __init__(self, player_id, C=1.41, Lambda=0.01):\n",
        "        self.N = defaultdict(int) # total visits of each node\n",
        "        self.Q = defaultdict(int)  # total reward of each node\n",
        "        self.children = defaultdict(list)\n",
        "        self.H = defaultdict(float)\n",
        "        self.player_id = player_id\n",
        "        self.C = C\n",
        "        self.Lambda = Lambda\n",
        "\n",
        "    def choose(self, state):\n",
        "        node = Node(state, [], None)\n",
        "\n",
        "        if node not in self.children:\n",
        "            index = int(len(state.available_actions) * random.random())\n",
        "\n",
        "            return state.available_actions[index]\n",
        "\n",
        "        def score(n):\n",
        "            if self.N[n] == 0:\n",
        "                return float(\"-inf\")  # avoid unseen moves\n",
        "            return self.Q[n] / self.N[n]  # average reward\n",
        "\n",
        "        return max(self.children[node], key=score).actions[-1]\n",
        "\n",
        "    def search(self, state):\n",
        "        node = Node(state, [], None)\n",
        "\n",
        "        path, new_state = self._select(node)\n",
        "        leaf = path[-1]\n",
        "\n",
        "        self._expand_with_ordering(leaf, state, new_state)\n",
        "        reward = self._p_simulate(new_state, 0.1)\n",
        "            \n",
        "        self._backpropogate(path, reward)\n",
        "\n",
        "    def _select(self, node):\n",
        "        path = []\n",
        "\n",
        "        state_copy = node.state.clone()\n",
        "\n",
        "        while True:\n",
        "            path.append(node)\n",
        "\n",
        "            if node not in self.children or not self.children[node] or state_copy.winner is not None:\n",
        "                return path, state_copy\n",
        "\n",
        "            unexplored = [item for item in self.children[node] if item not in self.children.keys()]\n",
        "\n",
        "            if unexplored:\n",
        "                n = unexplored.pop()\n",
        "                state_copy.act(n.actions[-1])\n",
        "                path.append(n)\n",
        "                return path, state_copy\n",
        "\n",
        "            node = self._progressive_uct_select(node)          \n",
        "            state_copy.act(node.actions[-1])\n",
        "            \n",
        "    def _expand_with_ordering(self, node, root_state, new_state):\n",
        "        if node in self.children:\n",
        "            return\n",
        "\n",
        "        children = []\n",
        "\n",
        "        for action in new_state.available_actions:\n",
        "            if action.type == ActionType.SUMMON or action.type == ActionType.USE:\n",
        "                children.append(Node(root_state, node.actions + [action], node))\n",
        "\n",
        "        if children == []:\n",
        "            for action in new_state.available_actions:\n",
        "                children.append(Node(root_state, node.actions + [action], node))\n",
        "            \n",
        "\n",
        "        self.children[node] = children\n",
        "\n",
        "    def _p_simulate(self, new_state, p):\n",
        "        amount_deck = len(new_state.opposing_player.deck)\n",
        "        amount_hand = len(new_state.opposing_player.hand)\n",
        "\n",
        "        ids = map(attrgetter('instance_id'),\n",
        "                  new_state.opposing_player.hand +\n",
        "                  new_state.opposing_player.deck)\n",
        "\n",
        "        new_deck = []\n",
        "\n",
        "        for i, id in zip(range(amount_deck + amount_hand), ids):\n",
        "            random_index = int(3 * random.random())\n",
        "\n",
        "            card = new_state._draft_cards[i][random_index].make_copy(id)\n",
        "\n",
        "            new_deck.append(card)\n",
        "\n",
        "        random.shuffle(new_deck)\n",
        "\n",
        "        new_state.opposing_player.deck = new_deck\n",
        "        new_state.opposing_player.hand = []\n",
        "\n",
        "        new_state.opposing_player.draw(amount=amount_hand)\n",
        "\n",
        "        for player in new_state.players:\n",
        "            random.shuffle(player.deck)\n",
        "\n",
        "        agent = RandomBattleAgent()\n",
        "        \n",
        "        while True:\n",
        "            if new_state.winner is not None:\n",
        "                return 1 if new_state.winner == self.player_id else -1\n",
        "            action = agent.act(new_state)\n",
        "            new_state.act(action)\n",
        "            if action.type == ActionType.PASS and random.random() < p:\n",
        "                break\n",
        "\n",
        "        while new_state.current_player.id == self.player_id:\n",
        "            if new_state.winner is not None:\n",
        "                return 1\n",
        "            action = agent.act(new_state)\n",
        "            new_state.act(action)\n",
        "            \n",
        "        return 1 if eval_state(new_state, new_state.opposing_player, new_state.current_player) > eval_state(new_state, new_state.current_player, new_state.opposing_player) else -1\n",
        "        \n",
        "    def _backpropogate(self, path, reward):\n",
        "        for node in reversed(path):\n",
        "            self.N[node] += 1\n",
        "\n",
        "            if node.player_id == self.player_id:\n",
        "                self.Q[node] += reward\n",
        "            else:\n",
        "                self.Q[node] -= reward\n",
        "\n",
        "    def _progressive_uct_select(self, node):\n",
        "        log_n_vertex = math.log(self.N[node])\n",
        "        def uct(n):\n",
        "            if n not in self.H:\n",
        "                self.H[n] = eval_node(n)\n",
        "            return (self.Q[n] / self.N[n]) + self.C * math.sqrt(log_n_vertex / self.N[n]) + self.Lambda * (self.H[n] / (self.N[n] + 1))\n",
        "\n",
        "        return max(self.children[node], key=uct)\n",
        "        \n",
        "class BestMCTSBattleAgent():\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.total = 0\n",
        "\n",
        "    def seed(self, seed):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def avg_simulations_per_second(self):\n",
        "        return self.count / self.total\n",
        "\n",
        "    def act(self, state, time_limit_ms=1000):\n",
        "        searcher = BestMCTS(state.current_player.id)\n",
        "        \n",
        "        if len(state.available_actions) == 1:\n",
        "            return state.available_actions[0]\n",
        "        \n",
        "        start_time = int(time.time() * 1000.0)\n",
        "        while True:\n",
        "            current_time = int(time.time() * 1000.0)\n",
        "\n",
        "            if current_time - start_time > time_limit_ms:\n",
        "                break\n",
        "            \n",
        "            searcher.search(state)\n",
        "            self.count += 1\n",
        "        self.total += 1\n",
        "        return searcher.choose(state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp3udN1JLoYS"
      },
      "source": [
        "*some additional helper functions for the deep RL agents*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck0C8D9CLuO3"
      },
      "source": [
        "name = \"LOCM-battle-v0\"\n",
        "env = gym.make(name)\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "observation_space_shape = np.array(env.observation_space.shape).prod()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def available_actions():\n",
        "  available_actions = env.state.available_actions\n",
        "  action_numbers = []\n",
        "\n",
        "  for action_number in range(n_actions):\n",
        "    action = env.decode_action(action_number)\n",
        "    if action in available_actions:\n",
        "      action_numbers.append(action_number)\n",
        "\n",
        "  return np.array(action_numbers, dtype=np.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GcRQHGNBcz"
      },
      "source": [
        "**Q Network agent** - deep RL agent that uses a neural network for the play policy (takes argmax of network output)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXO2LTaOshyW"
      },
      "source": [
        "class QNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(QNetwork, self).__init__()\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(observation_space_shape, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, n_actions)\n",
        "    )\n",
        "  \n",
        "  def forward(self, x, mask):\n",
        "    x = x.view(x.size(0),-1)\n",
        "    fx = x.float() / 256\n",
        "    q_vals_v = self.fc(fx)\n",
        "\n",
        "    masked_q_vals_v = q_vals_v.masked_fill(~mask, -np.inf)\n",
        "    return masked_q_vals_v\n",
        "\n",
        "class QNetworkBattleAgent():\n",
        "  def __init__(self, path):\n",
        "    self.net = QNetwork().to(device)\n",
        "    params = torch.load(path, map_location=torch.device(device))\n",
        "    self.net.load_state_dict(params['net'])\n",
        "    print('[Q Network]: Loaded trained model network from '+path)\n",
        "\n",
        "  def seed(self, seed):\n",
        "    pass\n",
        "\n",
        "  def reset(self):\n",
        "    pass\n",
        "\n",
        "  def act(self, state):\n",
        "    state = env.encode_state()\n",
        "\n",
        "    indices = torch.tensor(available_actions()).to(device)\n",
        "    mask = torch.zeros(n_actions, dtype=torch.bool).to(device)\n",
        "    mask.scatter_(0, indices, True)\n",
        "\n",
        "    state_a = np.array([state], copy=False)\n",
        "    state_v = torch.tensor(state_a).to(device)\n",
        "\n",
        "    q_vals_v = self.net(state_v, mask)\n",
        "    act_v = q_vals_v.argmax()\n",
        "    action = int(act_v.item())\n",
        "\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ptY4Ud7NUXA"
      },
      "source": [
        "**Policy network agent** - deep RL agent that uses a neural network for the play policy (samples actions to play from a discrete distribution constructed by softmaxing network outputs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvyPqBdyMuCT"
      },
      "source": [
        "class PolicyNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PolicyNet, self).__init__()\n",
        "\n",
        "    self.actor = nn.Sequential(\n",
        "      nn.Linear(observation_space_shape, 512),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(512, 256),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(256, n_actions)\n",
        "    )\n",
        "    self.critic = nn.Sequential(\n",
        "      nn.Linear(observation_space_shape, 512),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(512, 256),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(256, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = x.view(x.size(0),-1)\n",
        "    fx = x.float() / 256\n",
        "\n",
        "    policy = self.actor(fx)\n",
        "    value = self.critic(fx)\n",
        "\n",
        "    masked_policy = policy.masked_fill(~mask, -np.inf)\n",
        "    return masked_policy, value\n",
        "\n",
        "class PolicyNetBattleAgent():\n",
        "  def __init__(self, path):\n",
        "    self.net = PolicyNet().to(device)\n",
        "    params = torch.load(path, map_location=torch.device(device))\n",
        "    self.net.load_state_dict(params['net'])\n",
        "    print('[POLICY NET]: Loaded trained network from '+path)\n",
        "\n",
        "  def seed(self, seed):\n",
        "    pass\n",
        "\n",
        "  def reset(self):\n",
        "    pass\n",
        "\n",
        "  def act(self, state):\n",
        "    state = env.encode_state()\n",
        "\n",
        "    indices = torch.tensor(available_actions()).to(device)\n",
        "    mask = torch.zeros(n_actions dtype=torch.bool).to(device)\n",
        "    mask.scatter_(0, indices, True)\n",
        "\n",
        "    state_a = np.array([state], copy=False)\n",
        "    state_v = torch.tensor(state_a).to(device)\n",
        "\n",
        "    logits, _ = self.net(state_v, mask)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    dist = Categorical(probs)\n",
        "    action = int(dist.sample())\n",
        "\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CP-qzQ5Nka6"
      },
      "source": [
        "*Example code for playing games between AI agents*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1IedlEyvJOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985734d8-83f1-40bf-ea49-2cc6321c7f9f"
      },
      "source": [
        "name = \"LOCM-battle-v0\"\n",
        "env = gym.make(name, battle_agent=RandomBattleAgent())\n",
        "\n",
        "agent = # let this equal any of the agents above\n",
        "'''\n",
        "Examples:\n",
        "agent = GreedyBattleAgent()\n",
        "agent = QNetworkBattleAgent('../../../../drive/My Drive/...')\n",
        "'''\n",
        "\n",
        "num_episodes = 100\n",
        "total_wins = 0\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    while True:\n",
        "        action = agent.act(state)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        state = new_state\n",
        "        if done:\n",
        "            if reward == 1:\n",
        "                total_wins += 1\n",
        "            break\n",
        "env.close()\n",
        "\n",
        "print(\"Validation 1\")\n",
        "print(\"____ Agent vs ____ Agent\", str(total_wins / num_episodes * 100)+'% win rate')\n",
        "print(\"Execution Time\", (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded trained model from ../../../../drive/My Drive/training/locm-dqn-save-self3.chkpt\n",
            "Validation 1\n",
            "DQN Agent vs Random Agent 93.75% win rate\n",
            "Execution Time 413.63376688957214\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}